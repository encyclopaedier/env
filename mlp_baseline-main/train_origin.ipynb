{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from model import mlp\n",
    "def init_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "## 随机数种子的设置\n",
    "init_seeds(seed=666)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170206 finished\n",
      "20170110 finished\n",
      "20170118 finished\n",
      "20170123 finished\n",
      "20170116 finished\n",
      "20170203 finished\n",
      "20170119 finished\n",
      "20170209 finished\n",
      "20170207 finished\n",
      "20170103 finished\n",
      "20170217 finished\n",
      "20170104 2017011720170216finished  \n",
      "finishedfinished\n",
      "\n",
      "20170210 finished\n",
      "20170125 finished\n",
      "20170126 finished\n",
      "20170112 finished\n",
      "20170208 finished\n",
      "20170124 finished\n",
      "20170213 finished\n",
      "20170214 finished\n",
      "20170120 finished\n",
      "20170109 finished20170220 \n",
      "finished\n",
      "20170105 finished\n",
      "20170106 20170111finished \n",
      "finished\n",
      "2017022220170221  finishedfinished\n",
      "\n",
      "20170215 finished\n",
      "20170113 finished\n",
      "20170223 finished\n",
      "20170224 finished\n",
      "20170227 finished\n",
      "20170228 finished\n",
      "20170301 finished\n",
      "20170302 finished\n",
      "20170303 finished\n",
      "20170306 finished\n",
      "20170307 finished\n",
      "20170308 finished\n",
      "20170309 finished\n",
      "20170310 finished\n",
      "20170313 finished\n",
      "20170314 finished\n",
      "20170315 finished\n",
      "20170316 finished\n",
      "20170317 finished\n",
      "20170320 finished\n",
      "20170321 finished\n",
      "20170322 finished\n",
      "20170323 finished\n",
      "20170324 finished\n",
      "20170327 finished\n",
      "20170328 finished\n",
      "20170329 finished\n",
      "20170330 finished\n",
      "20170331 finished\n",
      "20170405 finished\n",
      "20170406 finished\n",
      "20170410 finished\n",
      "20170407 finished\n",
      "20170411 finished\n",
      "20170412 finished\n",
      "20170413 finished\n",
      "20170414 finished\n",
      "20170417 finished\n",
      "20170418 finished\n",
      "20170419 finished\n",
      "20170420 finished\n",
      "20170421 finished\n",
      "20170424 finished\n",
      "20170425 finished\n",
      "20170426 finished\n",
      "20170427 finished\n",
      "20170428 finished\n",
      "20170502 finished\n",
      "20170503 finished\n",
      "20170504 finished\n",
      "20170505 finished\n",
      "20170508 finished\n",
      "20170509 finished\n",
      "20170510 finished\n",
      "20170511 finished\n",
      "20170512 finished\n",
      "20170515 finished\n",
      "20170516 finished\n",
      "20170517 finished\n",
      "20170518 finished\n",
      "20170519 finished\n",
      "20170522 finished\n",
      "20170523 finished\n",
      "20170524 finished\n",
      "20170525 finished\n",
      "20170526 finished\n",
      "20170531 finished\n",
      "20170601 finished\n",
      "20170602 finished\n",
      "20170605 finished\n",
      "20170606 finished\n",
      "20170607 finished\n",
      "20170608 finished\n",
      "20170609 finished\n",
      "20170612 finished\n",
      "20170613 finished\n",
      "20170614 finished\n",
      "20170615 finished\n",
      "20170616 finished\n",
      "20170619 finished\n",
      "20170620 finished\n",
      "20170621 finished\n",
      "20170622 finished\n",
      "20170623 finished\n",
      "20170626 finished\n",
      "20170627 finished\n",
      "20170628 finished\n",
      "20170629 finished\n",
      "20170630 finished\n",
      "20170703 finished\n",
      "20170704 finished\n",
      "20170705 finished\n",
      "20170706 finished\n",
      "20170707 finished\n",
      "20170710 finished\n",
      "20170711 finished\n",
      "20170712 finished\n",
      "20170713 finished\n",
      "20170714 finished\n",
      "20170717 finished\n",
      "20170718 finished\n",
      "20170719 finished\n",
      "20170720 finished\n",
      "20170721 finished\n",
      "20170724 finished\n",
      "20170725 finished\n",
      "20170726 finished\n",
      "20170727 finished\n",
      "20170728 finished\n",
      "20170731 finished\n",
      "20170801 finished\n",
      "20170802 finished\n",
      "20170803 finished\n",
      "20170804 finished\n",
      "20170807 finished\n",
      "20170808 finished\n",
      "20170809 finished\n",
      "20170810 finished\n",
      "20170811 finished\n",
      "20170814 finished\n",
      "20170815 finished\n",
      "20170816 finished\n",
      "20170817 finished\n",
      "20170818 finished\n",
      "20170821 finished\n",
      "20170822 finished\n",
      "20170823 finished\n",
      "20170824 finished\n",
      "20170825 finished\n",
      "20170828 finished\n",
      "20170829 finished\n",
      "20170830 finished\n",
      "20170831 finished\n",
      "20170901 finished\n",
      "20170904 finished\n",
      "20170905 finished\n",
      "20170906 finished\n",
      "20170907 finished\n",
      "20170908 finished\n",
      "20170911 finished\n",
      "20170912 finished\n",
      "20170913 finished\n",
      "20170914 finished\n",
      "20170915 finished\n",
      "20170918 finished\n",
      "20170919 finished\n",
      "20170920 finished\n",
      "20170921 finished\n",
      "20170922 finished\n",
      "20170925 finished\n",
      "20170926 finished\n",
      "20170927 finished\n",
      "20170928 finished\n",
      "20170929 finished\n",
      "20171009 finished\n",
      "20171010 finished\n",
      "20171011 finished\n",
      "20171012 finished\n",
      "20171013 finished\n",
      "20171016 finished\n",
      "20171017 finished\n",
      "20171018 finished\n",
      "20171019 finished\n",
      "20171020 finished\n",
      "20171023 finished\n",
      "20171024 finished\n",
      "20171025 finished\n",
      "20171026 finished\n",
      "20171027 finished\n",
      "20171030 finished\n",
      "20171031 finished\n",
      "20171101 finished\n",
      "20171102 finished\n",
      "20171103 finished\n",
      "20171106 finished\n",
      "20171107 finished\n",
      "20171108 finished\n",
      "20171109 finished\n",
      "20171110 finished\n",
      "20171113 finished\n",
      "20171114 finished\n",
      "20171115 finished\n",
      "20171116 finished\n",
      "20171117 finished\n",
      "20171120 finished\n",
      "20171121 finished\n",
      "20171122 finished\n",
      "20171123 finished\n",
      "20171124 finished\n",
      "20171127 finished\n",
      "20171128 finished\n",
      "20171129 finished\n",
      "20171130 finished\n",
      "20171201 finished\n",
      "20171204 finished\n",
      "20171205 finished\n",
      "20171206 finished\n",
      "20171207 finished\n",
      "20171208 finished\n",
      "20171211 finished\n",
      "20171212 finished\n",
      "20171213 finished\n",
      "20171214 finished\n",
      "20171215 finished\n",
      "20171218 finished\n",
      "20171219 finished\n",
      "20171220 finished\n",
      "20171221 finished\n",
      "20171222 finished\n",
      "20171225 finished\n",
      "20171226 finished\n",
      "20171229 finished\n",
      "20171227 finished\n",
      "20171228 finished\n",
      "20180102 finished\n",
      "20180103 finished\n",
      "20180104 finished\n",
      "20180105 finished\n",
      "20180108 finished\n",
      "20180109 finished\n",
      "20180110 finished\n",
      "20180111 finished\n",
      "20180112 finished\n",
      "20180115 finished\n",
      "20180116 finished\n",
      "20180117 20180118finished \n",
      "finished\n",
      "20180119 finished\n",
      "20180122 finished\n",
      "20180123 finished\n",
      "20180124 finished\n",
      "20180125 finished\n",
      "20180126 finished\n",
      "20180129 finished\n",
      "20180130 finished\n",
      "20180131 finished\n",
      "20180201 finished\n",
      "20180202 finished\n",
      "20180205 finished\n",
      "20180206 finished\n",
      "20180208 finished\n",
      "20180207 finished\n",
      "20180209 finished\n",
      "20180212 finished\n",
      "20180213 finished\n",
      "20180214 finished\n",
      "20180222 finished\n",
      "20180223 finished\n",
      "20180226 finished\n",
      "20180227 finished\n",
      "20180228 finished\n",
      "20180301 finished\n",
      "20180302 finished\n",
      "20180305 finished\n",
      "20180306 finished\n",
      "20180307 finished\n",
      "20180308 finished\n",
      "20180309 finished\n",
      "20180312 finished\n",
      "20180313 finished\n",
      "20180314 finished\n",
      "20180315 finished\n",
      "20180316 finished\n",
      "20180319 finished\n",
      "20180320 finished\n",
      "20180321 finished\n",
      "20180322 finished\n",
      "20180323 finished\n",
      "20180326 finished\n",
      "20180327 finished\n",
      "20180328 finished\n",
      "20180329 finished\n",
      "20180330 finished\n",
      "20180402 finished\n",
      "20180403 finished\n",
      "20180409 finished\n",
      "20180404 finished\n",
      "20180410 finished\n",
      "20180411 finished\n",
      "20180412 finished\n",
      "20180413 finished\n",
      "20180416 finished\n",
      "20180417 finished\n",
      "20180418 finished\n",
      "20180419 finished\n",
      "20180420 finished\n",
      "20180423 finished\n",
      "20180424 finished\n",
      "20180425 finished\n",
      "20180426 finished\n",
      "20180427 finished\n",
      "20180502 finished\n",
      "20180503 finished\n",
      "20180504 finished\n",
      "20180507 finished\n",
      "20180508 finished\n",
      "20180509 finished\n",
      "20180510 finished\n",
      "20180511 finished\n",
      "20180514 finished\n",
      "20180515 finished\n",
      "20180516 finished\n",
      "20180517 finished\n",
      "20180518 finished\n",
      "20180521 finished\n",
      "20180522 finished\n",
      "20180523 finished\n",
      "20180525 finished\n",
      "20180524 finished\n",
      "20180528 finished\n",
      "20180529 finished\n",
      "20180530 finished\n",
      "20180531 finished\n",
      "20180601 finished\n",
      "20180604 finished\n",
      "20180605 finished\n",
      "20180606 finished\n",
      "20180607 finished\n",
      "20180608 finished\n",
      "20180611 finished\n",
      "20180612 finished\n",
      "20180613 finished\n",
      "20180614 finished\n",
      "20180615 finished\n",
      "20180619 finished\n",
      "20180620 finished\n",
      "20180621 finished\n",
      "20180622\n",
      " finished20180625 finished\n",
      "20180626 finished\n",
      "20180627 finished\n",
      "20180628 finished\n",
      "20180629 finished\n",
      "20180702 finished\n",
      "20180703 finished\n",
      "20180704 finished\n",
      "20180705 finished\n",
      "20180706 finished\n",
      "20180709 finished\n",
      "20180710 finished\n",
      "20180711 finished\n",
      "20180712 finished\n",
      "20180713 finished\n",
      "20180716 finished\n",
      "20180717 finished\n",
      "20180718 finished\n",
      "20180719 finished\n",
      "20180720 finished\n",
      "20180723 finished\n",
      "20180724 finished\n",
      "20180725 finished\n",
      "20180726 finished\n",
      "20180727 finished\n",
      "20180730 finished\n",
      "20180731 finished\n",
      "20180801 finished\n",
      "20180802 finished\n",
      "20180803 finished\n",
      "20180806 finished\n",
      "20180807 finished\n",
      "20180808 finished\n",
      "20180809 finished\n",
      "20180810 finished\n",
      "20180813 finished\n",
      "20180814 finished\n",
      "20180815 finished\n",
      "20180816 finished\n",
      "20180817 finished\n",
      "20180820 finished\n",
      "20180821 finished\n",
      "20180822 finished\n",
      "20180823 finished\n",
      "20180824 finished\n",
      "20180827 finished\n",
      "20180828 finished\n",
      "20180829 finished\n",
      "20180830 finished\n",
      "20180831 finished\n",
      "20180903 finished\n",
      "20180904 finished\n",
      "20180905 finished\n",
      "20180906 finished\n",
      "20180907 finished\n",
      "20180910 finished\n",
      "20180911 finished\n",
      "20180912 finished\n",
      "20180913 finished\n",
      "20180914 finished\n",
      "20180917 finished\n",
      "20180918 finished\n",
      "20180919 finished\n",
      "20180920 finished\n",
      "20180921 finished\n",
      "20180925 finished\n",
      "20180926 finished\n",
      "20180927 finished\n",
      "20180928 finished\n",
      "20181008 finished\n",
      "20181009 finished\n",
      "20181010 finished\n",
      "20181011 finished\n",
      "20181012 finished\n",
      "20181015 finished\n",
      "20181016 finished\n",
      "20181017 finished\n",
      "20181018 finished\n",
      "20181019 finished\n",
      "20181022 finished\n",
      "20181023 finished\n",
      "20181024 finished\n",
      "20181025 finished\n",
      "20181026 finished\n",
      "20181029 finished\n",
      "20181030 finished\n",
      "20181031 finished\n",
      "20181101 finished\n",
      "20181102 finished\n",
      "20181105 finished\n",
      "20181106 finished\n",
      "20181107 finished\n",
      "20181108 finished\n",
      "20181109 finished\n",
      "20181112 finished\n",
      "20181113 finished\n",
      "20181114 finished\n",
      "20181115 finished\n",
      "20181116 finished\n",
      "20181119 finished\n",
      "20181120 finished\n",
      "20181121 finished\n",
      "20181122 finished\n",
      "20181123 finished\n",
      "20181126 finished\n",
      "20181127 finished\n",
      "20181128 finished\n",
      "20181129 finished\n",
      "20181130 finished\n",
      "20181203 finished\n",
      "20181204 finished\n",
      "20181205 finished\n",
      "20181206 finished\n",
      "20181207 finished\n",
      "20181210 finished\n",
      "20181211 finished\n",
      "20181212 finished\n",
      "20181213 finished\n",
      "20181214 finished\n",
      "20181217 finished\n",
      "20181218 finished\n",
      "20181219 finished\n",
      "20181220 finished\n",
      "20181221 finished\n",
      "20181224 finished\n",
      "20181225 finished\n",
      "20181226 finished\n",
      "20181227 finished\n",
      "20181228 finished\n",
      "20190102 finished\n",
      "20190103 finished\n",
      "20190104 finished\n",
      "20190107 finished\n",
      "20190108 finished\n",
      "20190109 finished\n",
      "20190110 finished\n",
      "20190111 finished\n",
      "20190114 finished\n",
      "20190115 finished\n",
      "20190116 finished\n",
      "20190117 finished\n",
      "20190118 finished\n",
      "20190121 finished\n",
      "20190122 finished\n",
      "20190123 finished\n",
      "20190124 finished\n",
      "20190125 finished\n",
      "20190128 finished\n",
      "20190129 finished\n",
      "20190130 finished\n",
      "20190131 finished\n",
      "20190201 finished\n",
      "20190211 finished\n",
      "20190212 finished\n",
      "20190213 finished\n",
      "20190214 finished\n",
      "20190215 finished\n",
      "20190218 finished\n",
      "20190219 finished\n",
      "20190220 finished\n",
      "20190221 finished\n",
      "20190222 finished\n",
      "20190225 finished\n",
      "20190226 finished\n",
      "20190227 finished\n",
      "20190228 finished\n",
      "20190301 finished\n",
      "20190304 finished\n",
      "20190305 finished\n",
      "20190306 finished\n",
      "20190307 finished\n",
      "20190308 finished\n",
      "20190311 finished\n",
      "20190312 finished\n",
      "20190313 finished\n",
      "20190314 finished\n",
      "20190315 finished\n",
      "20190318 finished\n",
      "20190319 finished\n",
      "20190320 finished\n",
      "20190321 finished\n",
      "20190322 finished\n",
      "20190325 finished\n",
      "20190326 finished\n",
      "20190327 finished\n",
      "20190328 finished\n",
      "20190329 finished\n",
      "20190401 finished\n",
      "20190402 finished\n",
      "20190403 finished\n",
      "20190404 finished\n",
      "20190408 finished\n",
      "20190409 finished\n",
      "20190410 finished\n",
      "20190411 finished\n",
      "20190412 finished\n",
      "20190415 finished\n",
      "20190416 finished\n",
      "20190417 finished\n",
      "20190418 finished\n",
      "20190419 finished\n",
      "20190422 finished\n",
      "20190423 finished\n",
      "20190424 finished\n",
      "20190425 finished\n",
      "20190426 finished\n",
      "20190429 finished\n",
      "20190430 finished\n",
      "20190506 finished\n",
      "20190507 finished\n",
      "20190508 finished\n",
      "20190509 finished\n",
      "20190510 finished\n",
      "20190513 finished\n",
      "20190514 finished\n",
      "20190515 finished\n",
      "20190516 finished\n",
      "20190517 finished\n",
      "20190520 finished\n",
      "20190521 finished\n",
      "20190522 finished\n",
      "20190523 finished\n",
      "20190524 finished\n",
      "20190527 finished\n",
      "20190528 finished\n",
      "20190529 finished\n",
      "20190530 finished\n",
      "20190531 finished\n",
      "20190603 finished\n",
      "20190604 finished\n",
      "20190605 finished\n",
      "20190606 finished\n",
      "20190610 finished\n",
      "20190611 finished\n",
      "20190612 finished\n",
      "20190613 finished\n",
      "20190614 finished\n",
      "20190617 finished\n",
      "20190618 finished\n",
      "20190619 finished\n",
      "20190620 finished\n",
      "20190621 finished\n",
      "20190624 finished\n",
      "20190625 finished\n",
      "20190626 finished\n",
      "20190627 finished\n",
      "20190628 finished\n",
      "20190701 finished\n",
      "20190702 finished\n",
      "20190703 finished\n",
      "20190704 finished\n",
      "20190705\n",
      " finished20190708\n",
      " finished20190709 finished\n",
      "20190710 finished\n",
      "20190711 finished\n",
      "20190712 finished\n",
      "20190715 finished\n",
      "20190716 finished\n",
      "20190717 finished\n",
      "20190718\n",
      " finished20190719 finished\n",
      "20190722 finished\n",
      "20190723 finished\n",
      "20190725 finished\n",
      "20190724 finished\n",
      "20190726 finished\n",
      "20190729 finished\n",
      "20190730 finished\n",
      "20190731 finished\n",
      "20190801 finished\n",
      "20190802 finished\n",
      "20190805 finished\n",
      "20190806 finished\n",
      "20190807 finished\n",
      "20190808 finished\n",
      "20190809 finished\n",
      "20190812 finished\n",
      "20190813 finished\n",
      "20190814 finished\n",
      "20190815 finished\n",
      "20190816 finished\n",
      "20190819 finished\n",
      "20190820 finished\n",
      "20190821 finished\n",
      "20190822 finished\n",
      "20190823 finished\n",
      "20190826 finished\n",
      "20190827 finished\n",
      "20190828 finished\n",
      "20190829 finished\n",
      "20190830 finished\n",
      "20190902 finished\n",
      "20190903 finished\n",
      "20190904 finished\n",
      "20190905 finished\n",
      "20190906 finished\n",
      "20190909 finished\n",
      "20190910 finished\n",
      "20190911 finished\n",
      "20190912 finished\n",
      "20190916 finished\n",
      "20190917 finished\n",
      "20190918 finished\n",
      "20190919 finished\n",
      "20190920 finished\n",
      "20190923 finished\n",
      "20190924 finished\n",
      "20190925 finished\n",
      "20190926 finished\n",
      "20190927 finished\n",
      "20190930 finished\n",
      "20191008 finished\n",
      "20191009 finished\n",
      "20191010 finished\n",
      "20191011 finished\n",
      "20191014 finished\n",
      "20191015 finished\n",
      "20191016 finished\n",
      "20191017 finished\n",
      "20191018 finished\n",
      "20191021 finished\n",
      "20191022 finished\n",
      "20191023 finished\n",
      "20191024 finished\n",
      "20191025 finished\n",
      "20191028 finished\n",
      "20191029 finished\n",
      "20191030 finished\n",
      "20191031 finished\n",
      "20191101 finished\n",
      "20191104 finished\n",
      "20191105 finished\n",
      "20191106 finished\n",
      "20191107 finished\n",
      "20191108 finished\n",
      "20191111 finished\n",
      "20191112 finished\n",
      "20191113 finished\n",
      "20191114 finished\n",
      "20191115 finished\n",
      "20191118 finished\n",
      "20191119 finished\n",
      "20191120 finished\n",
      "20191121 finished\n",
      "20191122 finished\n",
      "20191125 finished\n",
      "20191126 finished\n",
      "20191127 finished\n",
      "20191128 finished\n",
      "20191129 finished\n",
      "20191202 finished\n",
      "20191203 finished\n",
      "20191204 finished\n",
      "20191205 finished\n",
      "20191206 finished\n",
      "20191209 finished\n",
      "20191210 finished\n",
      "20191211 finished\n",
      "20191212 finished\n",
      "20191213 finished\n",
      "20191216 finished\n",
      "20191217 finished\n",
      "20191218 finished\n",
      "20191219 finished\n",
      "20191220 finished\n",
      "20191223 finished\n",
      "20191224 finished\n",
      "20191225 finished\n",
      "20191226 finished\n",
      "20191227 finished\n",
      "20191230 finished\n",
      "20191231 finished\n",
      "20200102 finished\n",
      "20200103 finished\n",
      "20200106 finished\n",
      "20200107 finished\n",
      "20200108 finished\n",
      "20200109 finished\n",
      "20200110 finished\n",
      "20200113 finished\n",
      "20200114 finished\n",
      "20200115 finished\n",
      "20200116 finished\n",
      "20200117 finished\n",
      "20200120 finished\n",
      "20200121 finished\n",
      "20200203 finished\n",
      "20200122 finished\n",
      "20200123 finished\n",
      "20200204 finished\n",
      "20200205 finished\n",
      "20200206 finished\n",
      "20200207 finished\n",
      "20200210 finished\n",
      "20200211 finished\n",
      "20200213 finished20200212\n",
      " finished\n",
      "20200214 finished\n",
      "20200217 finished\n",
      "20200218 finished\n",
      "20200219 finished\n",
      "20200220 finished\n",
      "20200221 finished\n",
      "20200224 finished\n",
      "20200225 finished\n",
      "20200226 finished\n",
      "20200227 finished\n",
      "20200228 finished\n",
      "20200302 finished\n",
      "20200303 finished\n",
      "20200304 finished\n",
      "20200305 finished\n",
      "20200306 finished\n",
      "20200309 finished\n",
      "20200310 finished\n",
      "20200311 finished\n",
      "20200312 finished\n",
      "20200313 finished\n",
      "20200316 finished\n",
      "20200317 finished\n",
      "20200318 finished\n",
      "20200319 finished\n",
      "20200320 finished\n",
      "20200323 finished\n",
      "20200324 finished\n",
      "20200325 finished\n",
      "20200326 finished\n",
      "20200327 finished\n",
      "20200330 finished\n",
      "20200331 finished\n",
      "20200401 finished\n",
      "20200402 finished\n",
      "20200403 finished\n",
      "20200407 finished\n",
      "20200408 finished\n",
      "20200409 finished\n",
      "20200410 finished\n",
      "20200413 finished\n",
      "20200414 finished\n",
      "20200415 finished\n",
      "20200416 finished\n",
      "20200417 finished\n",
      "20200420 finished\n",
      "20200421 finished\n",
      "20200422 finished\n",
      "20200423 finished\n",
      "20200424 finished\n",
      "20200427 finished\n",
      "20200428 finished\n",
      "20200429 finished\n",
      "20200430 finished\n",
      "20200506 finished\n",
      "20200507 finished\n",
      "20200508 finished\n",
      "20200511 finished\n",
      "20200512 finished\n",
      "20200513 finished\n",
      "20200514 finished\n",
      "20200515 finished\n",
      "20200518 finished\n",
      "20200519 finished\n",
      "20200520\n",
      " finished20200521 finished\n",
      "20200522 finished\n",
      "20200525 finished\n",
      "20200526 finished\n",
      "20200527 finished\n",
      "20200528 finished\n",
      "20200529 finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>ticker</th>\n",
       "      <th>m1_ts_eod_rank_group</th>\n",
       "      <th>x_297</th>\n",
       "      <th>x_1185</th>\n",
       "      <th>x_2034</th>\n",
       "      <th>x_963</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_24</th>\n",
       "      <th>...</th>\n",
       "      <th>x_2079</th>\n",
       "      <th>x_669</th>\n",
       "      <th>x_1313</th>\n",
       "      <th>x_1370</th>\n",
       "      <th>x_2169</th>\n",
       "      <th>x_884</th>\n",
       "      <th>m1_ts_y_60twap_2n1open_Wgted_fullmarket_ex</th>\n",
       "      <th>m1_ts_y_60twap_2n2open_Wgted_fullmarket_ex</th>\n",
       "      <th>m1_ts_y_60twap_2n3open_Wgted_fullmarket_ex</th>\n",
       "      <th>y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170103</td>\n",
       "      <td>93500000</td>\n",
       "      <td>000001</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.244516</td>\n",
       "      <td>-0.257979</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507487</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.067372</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-1.691355</td>\n",
       "      <td>-0.006202</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>-0.013787</td>\n",
       "      <td>0.389313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170103</td>\n",
       "      <td>93500000</td>\n",
       "      <td>000002</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.244516</td>\n",
       "      <td>-1.172740</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.172024</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.067372</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-1.691355</td>\n",
       "      <td>-0.011067</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>-0.012301</td>\n",
       "      <td>0.160305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170103</td>\n",
       "      <td>93500000</td>\n",
       "      <td>000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.244516</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489318</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.067372</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-1.691355</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>-0.005595</td>\n",
       "      <td>-0.014103</td>\n",
       "      <td>0.247148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170103</td>\n",
       "      <td>93500000</td>\n",
       "      <td>000005</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.244516</td>\n",
       "      <td>-1.172740</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.172024</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.067372</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-1.691355</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.809160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170103</td>\n",
       "      <td>93500000</td>\n",
       "      <td>000006</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.244516</td>\n",
       "      <td>-0.396293</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655452</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.067372</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-1.691355</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>-0.005131</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.381679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101542504</th>\n",
       "      <td>20200529</td>\n",
       "      <td>144500000</td>\n",
       "      <td>688388</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.659620</td>\n",
       "      <td>-0.133213</td>\n",
       "      <td>-0.441238</td>\n",
       "      <td>0.333571</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>1.638251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341353</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>-0.174547</td>\n",
       "      <td>0.557318</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.697092</td>\n",
       "      <td>-0.007443</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>-0.004652</td>\n",
       "      <td>0.300366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101542505</th>\n",
       "      <td>20200529</td>\n",
       "      <td>144500000</td>\n",
       "      <td>688389</td>\n",
       "      <td>0</td>\n",
       "      <td>3.399152</td>\n",
       "      <td>2.876295</td>\n",
       "      <td>-0.441238</td>\n",
       "      <td>-1.353063</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.876064</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652258</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>0.779824</td>\n",
       "      <td>0.557318</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.697092</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>-0.013188</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.745520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101542506</th>\n",
       "      <td>20200529</td>\n",
       "      <td>144500000</td>\n",
       "      <td>688396</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.441238</td>\n",
       "      <td>-0.782245</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.360456</td>\n",
       "      <td>...</td>\n",
       "      <td>1.319989</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>-0.449481</td>\n",
       "      <td>0.557318</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.697092</td>\n",
       "      <td>0.025772</td>\n",
       "      <td>0.050579</td>\n",
       "      <td>0.084653</td>\n",
       "      <td>0.892086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101542507</th>\n",
       "      <td>20200529</td>\n",
       "      <td>144500000</td>\n",
       "      <td>688398</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.136792</td>\n",
       "      <td>-0.441238</td>\n",
       "      <td>1.233602</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.437993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.153855</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>-0.993084</td>\n",
       "      <td>0.557318</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.697092</td>\n",
       "      <td>-0.008992</td>\n",
       "      <td>-0.024924</td>\n",
       "      <td>-0.037442</td>\n",
       "      <td>0.200717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101542508</th>\n",
       "      <td>20200529</td>\n",
       "      <td>144500000</td>\n",
       "      <td>688399</td>\n",
       "      <td>2</td>\n",
       "      <td>0.807827</td>\n",
       "      <td>0.155142</td>\n",
       "      <td>-0.441238</td>\n",
       "      <td>0.141762</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>1.366428</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150661</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>1.192947</td>\n",
       "      <td>0.557318</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.697092</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.028514</td>\n",
       "      <td>-0.020290</td>\n",
       "      <td>0.745520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101542509 rows × 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  TimeStamp  ticker  m1_ts_eod_rank_group     x_297  \\\n",
       "0          20170103   93500000  000001                     9  0.000000   \n",
       "1          20170103   93500000  000002                     9  0.000000   \n",
       "2          20170103   93500000  000004                     2  0.000000   \n",
       "3          20170103   93500000  000005                     5  0.000000   \n",
       "4          20170103   93500000  000006                     8  0.000000   \n",
       "...             ...        ...     ...                   ...       ...   \n",
       "101542504  20200529  144500000  688388                     3 -0.659620   \n",
       "101542505  20200529  144500000  688389                     0  3.399152   \n",
       "101542506  20200529  144500000  688396                     7  0.000000   \n",
       "101542507  20200529  144500000  688398                     0  0.716784   \n",
       "101542508  20200529  144500000  688399                     2  0.807827   \n",
       "\n",
       "             x_1185    x_2034     x_963      x_34      x_24  ...    x_2079  \\\n",
       "0          0.000000  2.244516 -0.257979  0.085926  0.000000  ... -0.507487   \n",
       "1          0.000000  2.244516 -1.172740  0.085926  0.000000  ... -1.172024   \n",
       "2          0.000000  2.244516  0.011106  0.085926  0.000000  ...  0.489318   \n",
       "3          0.000000  2.244516 -1.172740  0.085926  0.000000  ... -1.172024   \n",
       "4          0.000000  2.244516 -0.396293  0.085926  0.000000  ...  0.655452   \n",
       "...             ...       ...       ...       ...       ...  ...       ...   \n",
       "101542504 -0.133213 -0.441238  0.333571  0.018242  1.638251  ... -0.341353   \n",
       "101542505  2.876295 -0.441238 -1.353063  0.018242  0.876064  ...  1.652258   \n",
       "101542506  0.000000 -0.441238 -0.782245  0.018242  0.360456  ...  1.319989   \n",
       "101542507  0.136792 -0.441238  1.233602  0.018242  0.437993  ...  1.153855   \n",
       "101542508  0.155142 -0.441238  0.141762  0.018242  1.366428  ...  2.150661   \n",
       "\n",
       "              x_669    x_1313    x_1370    x_2169     x_884  \\\n",
       "0          0.125797 -0.067372  0.082170 -0.000025 -1.691355   \n",
       "1          0.125797 -0.067372  0.082170 -0.000025 -1.691355   \n",
       "2          0.125797 -0.067372  0.082170 -0.000025 -1.691355   \n",
       "3          0.125797 -0.067372  0.082170 -0.000025 -1.691355   \n",
       "4          0.125797 -0.067372  0.082170 -0.000025 -1.691355   \n",
       "...             ...       ...       ...       ...       ...   \n",
       "101542504  0.061437 -0.174547  0.557318 -0.000025  1.697092   \n",
       "101542505  0.061437  0.779824  0.557318 -0.000025  1.697092   \n",
       "101542506  0.061437 -0.449481  0.557318 -0.000025  1.697092   \n",
       "101542507  0.061437 -0.993084  0.557318 -0.000025  1.697092   \n",
       "101542508  0.061437  1.192947  0.557318 -0.000025  1.697092   \n",
       "\n",
       "           m1_ts_y_60twap_2n1open_Wgted_fullmarket_ex  \\\n",
       "0                                           -0.006202   \n",
       "1                                           -0.011067   \n",
       "2                                           -0.007151   \n",
       "3                                            0.005877   \n",
       "4                                           -0.005115   \n",
       "...                                               ...   \n",
       "101542504                                   -0.007443   \n",
       "101542505                                    0.004475   \n",
       "101542506                                    0.025772   \n",
       "101542507                                   -0.008992   \n",
       "101542508                                    0.004168   \n",
       "\n",
       "           m1_ts_y_60twap_2n2open_Wgted_fullmarket_ex  \\\n",
       "0                                           -0.015217   \n",
       "1                                           -0.004409   \n",
       "2                                           -0.005595   \n",
       "3                                            0.006150   \n",
       "4                                           -0.005131   \n",
       "...                                               ...   \n",
       "101542504                                    0.011940   \n",
       "101542505                                   -0.013188   \n",
       "101542506                                    0.050579   \n",
       "101542507                                   -0.024924   \n",
       "101542508                                   -0.028514   \n",
       "\n",
       "           m1_ts_y_60twap_2n3open_Wgted_fullmarket_ex        y1  \n",
       "0                                           -0.013787  0.389313  \n",
       "1                                           -0.012301  0.160305  \n",
       "2                                           -0.014103  0.247148  \n",
       "3                                            0.014996  0.809160  \n",
       "4                                            0.009620  0.381679  \n",
       "...                                               ...       ...  \n",
       "101542504                                   -0.004652  0.300366  \n",
       "101542505                                    0.017150  0.745520  \n",
       "101542506                                    0.084653  0.892086  \n",
       "101542507                                   -0.037442  0.200717  \n",
       "101542508                                   -0.020290  0.745520  \n",
       "\n",
       "[101542509 rows x 359 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/data/local_data/shared/102/intern_data_yzhou/zy4_parquet'\n",
    "mean_std_path = '/mnt/beegfs/strategy_intern/zzdai_intern/for_wpxu/data_processed/mean_std_all.pkl'\n",
    "feas = pd.read_pickle('/mnt/beegfs/strategy_intern/zzdai_intern/for_wpxu/feas353.pkl')\n",
    "feas = [i for i in feas if 'x_' in i]\n",
    "feas = [i for i in feas if 'x_1663' not in i]\n",
    "start_date = '20170101'\n",
    "end_date = '20200531'\n",
    "mean_std_df = pd.read_pickle(mean_std_path)\n",
    "means = mean_std_df.loc[:, 'mean'].astype(np.float32)\n",
    "stds = mean_std_df.loc[:, 'std']\n",
    "stds = stds.apply(lambda x: max(1e-10, x)).astype(np.float32)\n",
    "y_cols = ['m1_ts_y_60twap_2n1open_Wgted_fullmarket_ex', \n",
    "          'm1_ts_y_60twap_2n2open_Wgted_fullmarket_ex', \n",
    "          'm1_ts_y_60twap_2n3open_Wgted_fullmarket_ex']\n",
    "y_name = y_cols[0]\n",
    "def process_data_mp(data_path, feas, means, stds, y_cols, y_name, filters, date_start, date_end):\n",
    "    files = f'{data_path}/{date_start}.parquet'\n",
    "    dataset = pq.ParquetDataset(files, use_legacy_dataset=False, filters=filters)\n",
    "    _tmp_data = dataset.read(columns=['date', 'TimeStamp', 'ticker', 'm1_ts_eod_rank_group']+feas+y_cols).to_pandas()\n",
    "    _tmp_data.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "    _tmp_data['y1'] = _tmp_data.groupby(['date','TimeStamp', 'm1_ts_eod_rank_group'])[y_name].rank(axis=0, pct=True)\n",
    "    _tmp_data[feas] = (_tmp_data[feas] - means.loc[feas])/stds.loc[feas]\n",
    "    _tmp_data[feas] = _tmp_data[feas].fillna(0)\n",
    "    _tmp_data[feas] = _tmp_data[feas].astype(np.float32)\n",
    "    _tmp_data = _tmp_data.dropna()\n",
    "    print(date_start, 'finished')\n",
    "    return _tmp_data\n",
    "\n",
    "zt_filter = ('m1_ts_z_tag_up_limit','=',0.0)\n",
    "dt_filter = ('m1_ts_z_tag_down_limit','=',0.0)\n",
    "yna_filter = (y_name,'!=',np.nan)\n",
    "filters = [zt_filter, dt_filter, yna_filter]\n",
    "file_list = sorted(os.listdir(data_path))\n",
    "date_all = [i.split('.')[0] for i in file_list]\n",
    "date_list = [date for date in date_all if date>=start_date and date<=end_date]\n",
    "mul_dfs = []\n",
    "data_fea = []\n",
    "pool = multiprocessing.Pool(32)\n",
    "for i, dt in enumerate(date_list):\n",
    "    mul_dfs.append(pool.apply_async(process_data_mp, (data_path, feas, means, stds, y_cols, y_name, filters, dt, dt)))\n",
    "for item in mul_dfs:\n",
    "    data_fea.append(item.get())\n",
    "pool.close()\n",
    "data_fea = pd.concat(data_fea, ignore_index=True)\n",
    "data_fea = data_fea.sort_values(by=['date', 'TimeStamp', 'ticker'], ascending=True).reset_index(drop=True)\n",
    "data_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:20170103, 20200330\n",
      "valid:20200330, 20200529\n"
     ]
    }
   ],
   "source": [
    "data_train = data_fea.iloc[:int(0.95 * len(data_fea))].reset_index(drop=True)\n",
    "data_valid = data_fea.iloc[int(0.95 * len(data_fea)):].reset_index(drop=True)\n",
    "del data_fea\n",
    "print(f\"train:{data_train.iloc[0].date}, {data_train.iloc[-1].date}\")\n",
    "print(f\"valid:{data_valid.iloc[0].date}, {data_valid.iloc[-1].date}\")\n",
    "y_train = data_train[['date', \"TimeStamp\", y_name]].copy()\n",
    "y_valid = data_valid[['date', \"TimeStamp\", y_name]].copy()\n",
    "x_valid_tensor, y_valid_tensor = torch.tensor(np.array(data_valid[feas]), dtype=torch.float32), torch.tensor(np.array(data_valid['y1']), dtype=torch.float32)\n",
    "dataset_valid = TensorDataset(x_valid_tensor, y_valid_tensor)\n",
    "dataloader_valid  = DataLoader(dataset=dataset_valid, batch_size=2**14, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(y_pred, y_label, perc=3, y_name='m1_ts_y_60twap_2n1open_Wgted_fullmarket_ex'):\n",
    "    y_test = y_label.copy()\n",
    "    y_test['y_pred'] = y_pred                     \n",
    "    y_test['y_pred_bin'] = y_test.groupby(['date', \"TimeStamp\"])['y_pred'].transform(lambda x: x>=np.percentile(x, 100-perc))\n",
    "    y_metric = y_test.loc[y_test['y_pred_bin'].values, :].copy()\n",
    "    rtn = y_metric.groupby(['date', \"TimeStamp\"])[y_name].apply(np.mean).groupby(['date']).mean()    \n",
    "    return np.sum(rtn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(itrs, net, dataloader_train, optim, device, dataloader_valid, y_valid, experiment_path, lr_schedule_values=None, scheduler=None, interval=0, ema_net=None, regularization_strength=1, metric_type='sharpe', loss_type='mse'):\n",
    "    l_loss = []\n",
    "    metrics = []\n",
    "    net.train()\n",
    "    best_metric = -1e10 # 用来记录验证集最优的metric\n",
    "    global_steps = 0 # 用来记录迭代次数\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    for epoch_i in range(1000):\n",
    "        n = 0\n",
    "        start = time.time()\n",
    "        l = 0\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(dataloader_train):\n",
    "            global_steps += 1  # global training iteration\n",
    "            if global_steps > itrs:\n",
    "                break\n",
    "            x_batch  = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = net(x_batch)\n",
    "            loss = criterion(y_batch.view(-1), output.view(-1))\n",
    "                \n",
    "            loss.backward() # 反向传播\n",
    "            optim.step() # 参数更新\n",
    "            optim.zero_grad()\n",
    "            l += loss.item() * x_batch.shape[0]\n",
    "            n += x_batch.shape[0]\n",
    "            ## epoch内计算验证集的metric\n",
    "            if dataloader_valid is not None and global_steps % interval == 0 and global_steps>=600:\n",
    "                metric = valid(net, dataloader_valid, device, y_valid, metric_type)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    torch.save(net.state_dict(), os.path.join(experiment_path, 'best.pth'))\n",
    "                print(f\"epoch:{epoch_i}, batch:{batch_i}, loss:{loss.item()}\")\n",
    "                print(f\"itrs:{global_steps}, metric:{metric}\")\n",
    "        if global_steps > itrs:\n",
    "            break\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        l_loss.append(l/n)\n",
    "        print(f'loss:{l/n}') \n",
    "        print(f'cost:{time.time() - start}s') \n",
    "        \n",
    "    \n",
    "def valid(net, dataloader_valid, device, y_valid, metric_type='rtn', loss_type='mse'):\n",
    "    y_pred = torch.Tensor()\n",
    "    l = 0\n",
    "    metric = 0\n",
    "    n = 0\n",
    "    net.eval()  # 推理的时候需要将model的模式设置为eval，固定住BN层的均值方差\n",
    "    print(\"valid dataloader: \")\n",
    "    ll = []\n",
    "    if loss_type == 'mse':\n",
    "        criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(dataloader_valid):\n",
    "            bs = x_batch.shape[0]\n",
    "            n += bs\n",
    "            x_batch  = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = net(x_batch)\n",
    "            loss = criterion(y_batch.view(-1), output.view(-1))\n",
    "            l += loss.item() * bs\n",
    "            y_pred = torch.cat((y_pred, output.reshape(-1).to('cpu')), dim=0)\n",
    "            if batch_i % 100 == 0 or batch_i == len(dataloader_valid)-1:\n",
    "                print(f\"{batch_i}/{len(dataloader_valid)}\")\n",
    "        print(f'valid loss:{l/n}')\n",
    "        metric = my_metric(y_pred.detach().numpy(), y_valid)\n",
    "    net.train()\n",
    "    return metric\n",
    "\n",
    "def get_parameter_groups(model, weight_decay=0.0):\n",
    "    parameter_group_names = {}\n",
    "    parameter_group_vars = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # frozen weights\n",
    "        ## bias，BN参数不施加正则\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\"):\n",
    "            group_name = \"no_decay\"\n",
    "            this_weight_decay = 0.\n",
    "        else:\n",
    "            group_name = \"decay\"\n",
    "            this_weight_decay = weight_decay\n",
    "        \n",
    "        if group_name not in parameter_group_names:\n",
    "            parameter_group_names[group_name] = {\n",
    "                \"weight_decay\": this_weight_decay,\n",
    "                \"params\": [],\n",
    "            }\n",
    "            parameter_group_vars[group_name] = {\n",
    "                \"weight_decay\": this_weight_decay,\n",
    "                \"params\": [],\n",
    "            }\n",
    "        parameter_group_vars[group_name][\"params\"].append(param)\n",
    "        parameter_group_names[group_name][\"params\"].append(name)\n",
    "    return list(parameter_group_vars.values())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08165034925084405\n",
      "epoch:0, batch:599, loss:0.0791986808180809\n",
      "itrs:600, metric:0.09679195641319388\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0813148568360165\n",
      "epoch:0, batch:649, loss:0.07929404079914093\n",
      "itrs:650, metric:0.11565698495453237\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0812236293827595\n",
      "epoch:0, batch:699, loss:0.07813281565904617\n",
      "itrs:700, metric:0.10654773762257592\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08122950270331472\n",
      "epoch:0, batch:749, loss:0.07852187752723694\n",
      "itrs:750, metric:0.11117532188860019\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08121043403749974\n",
      "epoch:0, batch:799, loss:0.07816293835639954\n",
      "itrs:800, metric:0.11997165976123243\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0816427941561022\n",
      "epoch:0, batch:849, loss:0.07755187153816223\n",
      "itrs:850, metric:0.09474204686682443\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08186436674787044\n",
      "epoch:0, batch:899, loss:0.0781354159116745\n",
      "itrs:900, metric:0.1000618345221887\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08176462055346836\n",
      "epoch:0, batch:949, loss:0.07772844284772873\n",
      "itrs:950, metric:0.10703406709725034\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08222258047920561\n",
      "epoch:0, batch:999, loss:0.0779673308134079\n",
      "itrs:1000, metric:0.10885956844229003\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0816349228426266\n",
      "epoch:0, batch:1049, loss:0.0772438496351242\n",
      "itrs:1050, metric:0.12341531051747047\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08221548195963324\n",
      "epoch:0, batch:1099, loss:0.07722459733486176\n",
      "itrs:1100, metric:0.11848929861950334\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08187681473168464\n",
      "epoch:0, batch:1149, loss:0.07733303308486938\n",
      "itrs:1150, metric:0.11629702862201535\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08350653043590084\n",
      "cost:1674.324806213379s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08316102269288353\n",
      "epoch:1, batch:21, loss:0.07666302472352982\n",
      "itrs:1200, metric:0.1354569882472185\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0823919368706303\n",
      "epoch:1, batch:71, loss:0.07616834342479706\n",
      "itrs:1250, metric:0.11440107663058287\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08267398682768608\n",
      "epoch:1, batch:121, loss:0.07614180445671082\n",
      "itrs:1300, metric:0.12296880285525731\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08246461766433667\n",
      "epoch:1, batch:171, loss:0.07644805312156677\n",
      "itrs:1350, metric:0.13038414050184285\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08257669492702847\n",
      "epoch:1, batch:221, loss:0.07582051306962967\n",
      "itrs:1400, metric:0.1420024940304447\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08245263683980893\n",
      "epoch:1, batch:271, loss:0.07544848322868347\n",
      "itrs:1450, metric:0.1400953098106634\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08281984396765982\n",
      "epoch:1, batch:321, loss:0.07614558190107346\n",
      "itrs:1500, metric:0.1224883964788824\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08315345158232673\n",
      "epoch:1, batch:371, loss:0.07450844347476959\n",
      "itrs:1550, metric:0.13056466979314793\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0840044952594097\n",
      "epoch:1, batch:421, loss:0.07449693977832794\n",
      "itrs:1600, metric:0.12590763747042627\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08387915647244451\n",
      "epoch:1, batch:471, loss:0.07461849600076675\n",
      "itrs:1650, metric:0.13019112824138876\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08438488671451068\n",
      "epoch:1, batch:521, loss:0.07468757033348083\n",
      "itrs:1700, metric:0.11077378611886261\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08354377689151617\n",
      "epoch:1, batch:571, loss:0.07483510673046112\n",
      "itrs:1750, metric:0.1357439306440742\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08462204070460107\n",
      "epoch:1, batch:621, loss:0.0742456316947937\n",
      "itrs:1800, metric:0.13742108427079275\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08359369925477352\n",
      "epoch:1, batch:671, loss:0.0743400901556015\n",
      "itrs:1850, metric:0.13706148434491625\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08389345812589848\n",
      "epoch:1, batch:721, loss:0.0738750547170639\n",
      "itrs:1900, metric:0.13280386814384457\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0843736451174601\n",
      "epoch:1, batch:771, loss:0.07384428381919861\n",
      "itrs:1950, metric:0.14444378718893758\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08491128350111672\n",
      "epoch:1, batch:821, loss:0.07399319112300873\n",
      "itrs:2000, metric:0.1471852765193085\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08128686144330731\n",
      "epoch:0, batch:599, loss:0.07799211144447327\n",
      "itrs:600, metric:0.1197001793698089\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08152538685419772\n",
      "epoch:0, batch:649, loss:0.07804513722658157\n",
      "itrs:650, metric:0.11701295847181413\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08160669816691755\n",
      "epoch:0, batch:699, loss:0.07837390899658203\n",
      "itrs:700, metric:0.11505680394821967\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08166726049408743\n",
      "epoch:0, batch:749, loss:0.07740779221057892\n",
      "itrs:750, metric:0.12172705362830433\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08169330257431746\n",
      "epoch:0, batch:799, loss:0.07748289406299591\n",
      "itrs:800, metric:0.13461394339230737\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08200909837767487\n",
      "epoch:0, batch:849, loss:0.07744964212179184\n",
      "itrs:850, metric:0.11682603368176894\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08227304272154715\n",
      "epoch:0, batch:899, loss:0.07781288027763367\n",
      "itrs:900, metric:0.10428048992657245\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08283631910719344\n",
      "epoch:0, batch:949, loss:0.0769474059343338\n",
      "itrs:950, metric:0.10888243131292054\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0824007562324417\n",
      "epoch:0, batch:999, loss:0.07664219290018082\n",
      "itrs:1000, metric:0.1163744810346461\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08275104873871374\n",
      "epoch:0, batch:1049, loss:0.07661750167608261\n",
      "itrs:1050, metric:0.1215200876084004\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08250917332221498\n",
      "epoch:0, batch:1099, loss:0.07655887305736542\n",
      "itrs:1100, metric:0.13652800123850287\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08266028584092307\n",
      "epoch:0, batch:1149, loss:0.07627888023853302\n",
      "itrs:1150, metric:0.13231756002999948\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.0842963943977201\n",
      "cost:1469.6649713516235s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08336399430125341\n",
      "epoch:1, batch:21, loss:0.07591846585273743\n",
      "itrs:1200, metric:0.1256557526688185\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08333113741532194\n",
      "epoch:1, batch:71, loss:0.07582488656044006\n",
      "itrs:1250, metric:0.1108539722053272\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08339870189083635\n",
      "epoch:1, batch:121, loss:0.07604742050170898\n",
      "itrs:1300, metric:0.1194763895784844\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08339511040737291\n",
      "epoch:1, batch:171, loss:0.07578027248382568\n",
      "itrs:1350, metric:0.1224110982594488\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08327536922643673\n",
      "epoch:1, batch:221, loss:0.0752142071723938\n",
      "itrs:1400, metric:0.1451766647747819\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08381302802118977\n",
      "epoch:1, batch:271, loss:0.07486119866371155\n",
      "itrs:1450, metric:0.1321510007916105\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08376653102117777\n",
      "epoch:1, batch:321, loss:0.07464821636676788\n",
      "itrs:1500, metric:0.10133948582788435\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08417409071509292\n",
      "epoch:1, batch:371, loss:0.0748024433851242\n",
      "itrs:1550, metric:0.11573267292499168\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08344643275764145\n",
      "epoch:1, batch:421, loss:0.07463334500789642\n",
      "itrs:1600, metric:0.12281394723618683\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08417647963551429\n",
      "epoch:1, batch:471, loss:0.07420149445533752\n",
      "itrs:1650, metric:0.1318768264081781\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08389469089680984\n",
      "epoch:1, batch:521, loss:0.07464008033275604\n",
      "itrs:1700, metric:0.12350857680532953\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08407425082100438\n",
      "epoch:1, batch:571, loss:0.07421267032623291\n",
      "itrs:1750, metric:0.13701601733345242\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08524616369927629\n",
      "epoch:1, batch:621, loss:0.0741281509399414\n",
      "itrs:1800, metric:0.13603134790548918\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0845415635666699\n",
      "epoch:1, batch:671, loss:0.07380808144807816\n",
      "itrs:1850, metric:0.12139643235526212\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08490150676970712\n",
      "epoch:1, batch:721, loss:0.07332652807235718\n",
      "itrs:1900, metric:0.12022948763147573\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08428260074067946\n",
      "epoch:1, batch:771, loss:0.07358850538730621\n",
      "itrs:1950, metric:0.14227918164961012\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08549090048958405\n",
      "epoch:1, batch:821, loss:0.07316747307777405\n",
      "itrs:2000, metric:0.1396991343273928\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08110730299273265\n",
      "epoch:0, batch:599, loss:0.07925093919038773\n",
      "itrs:600, metric:0.13009045871133113\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08449648920339445\n",
      "epoch:0, batch:649, loss:0.08146034181118011\n",
      "itrs:650, metric:0.09848974478819501\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08140133943043294\n",
      "epoch:0, batch:699, loss:0.07930971682071686\n",
      "itrs:700, metric:0.12298291215135275\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08140271782147508\n",
      "epoch:0, batch:749, loss:0.07866188883781433\n",
      "itrs:750, metric:0.12355110513102165\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08199002305240066\n",
      "epoch:0, batch:799, loss:0.0788002610206604\n",
      "itrs:800, metric:0.11820188667714117\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0813752051685188\n",
      "epoch:0, batch:849, loss:0.0782587081193924\n",
      "itrs:850, metric:0.1288434483017934\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08182887959473784\n",
      "epoch:0, batch:899, loss:0.07798580080270767\n",
      "itrs:900, metric:0.12006734941104848\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08162729677394201\n",
      "epoch:0, batch:949, loss:0.07818343490362167\n",
      "itrs:950, metric:0.11965261014547524\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08215162264031653\n",
      "epoch:0, batch:999, loss:0.07797885686159134\n",
      "itrs:1000, metric:0.11097661288581043\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08195596744073749\n",
      "epoch:0, batch:1049, loss:0.07762227207422256\n",
      "itrs:1050, metric:0.1261760798864082\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08242302656676265\n",
      "epoch:0, batch:1099, loss:0.07754712551832199\n",
      "itrs:1100, metric:0.11596827718096277\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08183994414544837\n",
      "epoch:0, batch:1149, loss:0.07763690501451492\n",
      "itrs:1150, metric:0.1389579570999154\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08821294031276722\n",
      "cost:1545.2433626651764s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08232320161871083\n",
      "epoch:1, batch:21, loss:0.0770614743232727\n",
      "itrs:1200, metric:0.14099831496565052\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08242569605256204\n",
      "epoch:1, batch:71, loss:0.07749378681182861\n",
      "itrs:1250, metric:0.1263961880880655\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08368415729646017\n",
      "epoch:1, batch:121, loss:0.07844820618629456\n",
      "itrs:1300, metric:0.10476145003756553\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0823934462047937\n",
      "epoch:1, batch:171, loss:0.07785633206367493\n",
      "itrs:1350, metric:0.12511153882516937\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0830946403968095\n",
      "epoch:1, batch:221, loss:0.07637587189674377\n",
      "itrs:1400, metric:0.12926065523367675\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08299909030300612\n",
      "epoch:1, batch:271, loss:0.07625402510166168\n",
      "itrs:1450, metric:0.13133349253304105\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08690212674138816\n",
      "epoch:1, batch:321, loss:0.07756172120571136\n",
      "itrs:1500, metric:0.0879046676988815\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08253524593406972\n",
      "epoch:1, batch:371, loss:0.07623614370822906\n",
      "itrs:1550, metric:0.13047951244020117\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08280955501302603\n",
      "epoch:1, batch:421, loss:0.07659640908241272\n",
      "itrs:1600, metric:0.1184069220733717\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08217637692531818\n",
      "epoch:1, batch:471, loss:0.07628071308135986\n",
      "itrs:1650, metric:0.10537357265529863\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08314856180323182\n",
      "epoch:1, batch:521, loss:0.07617978751659393\n",
      "itrs:1700, metric:0.15524135473126568\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08383837873330054\n",
      "epoch:1, batch:571, loss:0.07600362598896027\n",
      "itrs:1750, metric:0.11242047715017077\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08287930292333288\n",
      "epoch:1, batch:621, loss:0.07561367750167847\n",
      "itrs:1800, metric:0.13698900775423353\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08352797525446383\n",
      "epoch:1, batch:671, loss:0.07577309757471085\n",
      "itrs:1850, metric:0.12579260051921606\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08377559196059181\n",
      "epoch:1, batch:721, loss:0.07545516639947891\n",
      "itrs:1900, metric:0.12022342964205311\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08417593312823993\n",
      "epoch:1, batch:771, loss:0.07567983865737915\n",
      "itrs:1950, metric:0.11679186231477716\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08410910119428774\n",
      "epoch:1, batch:821, loss:0.07463444024324417\n",
      "itrs:2000, metric:0.13464200236104362\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08223336647157228\n",
      "epoch:0, batch:599, loss:0.07864318788051605\n",
      "itrs:600, metric:0.11011094438015082\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08134844963447596\n",
      "epoch:0, batch:649, loss:0.07857257127761841\n",
      "itrs:650, metric:0.1252687425551004\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08110319971859353\n",
      "epoch:0, batch:699, loss:0.07789365202188492\n",
      "itrs:700, metric:0.12479016235659451\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0813567289143358\n",
      "epoch:0, batch:749, loss:0.07813499867916107\n",
      "itrs:750, metric:0.11735046568893494\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08253712358809316\n",
      "epoch:0, batch:799, loss:0.0778907835483551\n",
      "itrs:800, metric:0.09869317150728169\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08182854170956078\n",
      "epoch:0, batch:849, loss:0.0780799388885498\n",
      "itrs:850, metric:0.11665858706605327\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08185431549233675\n",
      "epoch:0, batch:899, loss:0.0775676965713501\n",
      "itrs:900, metric:0.12151646165577352\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08172937705901867\n",
      "epoch:0, batch:949, loss:0.07748845219612122\n",
      "itrs:950, metric:0.13428745591217595\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08212830373886694\n",
      "epoch:0, batch:999, loss:0.07731746137142181\n",
      "itrs:1000, metric:0.12000353891781339\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08182742002383202\n",
      "epoch:0, batch:1049, loss:0.07707113027572632\n",
      "itrs:1050, metric:0.11809133148033449\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0821753126779051\n",
      "epoch:0, batch:1099, loss:0.07687284797430038\n",
      "itrs:1100, metric:0.11878356266605705\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08230401669857382\n",
      "epoch:0, batch:1149, loss:0.07659640908241272\n",
      "itrs:1150, metric:0.11437586740290158\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08424667479792886\n",
      "cost:1590.4088551998138s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08216651592160384\n",
      "epoch:1, batch:21, loss:0.07677818834781647\n",
      "itrs:1200, metric:0.13123681585982216\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08213787088886708\n",
      "epoch:1, batch:71, loss:0.07671286165714264\n",
      "itrs:1250, metric:0.11383277454000462\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08298863383384575\n",
      "epoch:1, batch:121, loss:0.07564841210842133\n",
      "itrs:1300, metric:0.11983341311867199\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08337502558984625\n",
      "epoch:1, batch:171, loss:0.07587610185146332\n",
      "itrs:1350, metric:0.11401752944097501\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08236724425180686\n",
      "epoch:1, batch:221, loss:0.07649120688438416\n",
      "itrs:1400, metric:0.1359794788203342\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08404103774485035\n",
      "epoch:1, batch:271, loss:0.07621993869543076\n",
      "itrs:1450, metric:0.10917620225482601\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08364049209629228\n",
      "epoch:1, batch:321, loss:0.07586968690156937\n",
      "itrs:1500, metric:0.11521480051734317\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08432051376500577\n",
      "epoch:1, batch:371, loss:0.07554960250854492\n",
      "itrs:1550, metric:0.09248139010428626\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08325653813363303\n",
      "epoch:1, batch:421, loss:0.07551833242177963\n",
      "itrs:1600, metric:0.118634377459216\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08401926934516363\n",
      "epoch:1, batch:471, loss:0.07551044970750809\n",
      "itrs:1650, metric:0.0929433505494545\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08412299245435989\n",
      "epoch:1, batch:521, loss:0.07536859810352325\n",
      "itrs:1700, metric:0.11306043030657267\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08448087287447101\n",
      "epoch:1, batch:571, loss:0.07593002915382385\n",
      "itrs:1750, metric:0.10966351405671353\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08394550456148171\n",
      "epoch:1, batch:621, loss:0.07446084916591644\n",
      "itrs:1800, metric:0.11326848454368794\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08421081622263689\n",
      "epoch:1, batch:671, loss:0.07561568915843964\n",
      "itrs:1850, metric:0.10120896532713332\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0846256613971464\n",
      "epoch:1, batch:721, loss:0.07425060868263245\n",
      "itrs:1900, metric:0.10338653631193767\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08409573551561332\n",
      "epoch:1, batch:771, loss:0.07409022003412247\n",
      "itrs:1950, metric:0.10663426503319155\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08469634015255392\n",
      "epoch:1, batch:821, loss:0.0751408189535141\n",
      "itrs:2000, metric:0.10389513065703997\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08094838811746692\n",
      "epoch:0, batch:599, loss:0.0798671692609787\n",
      "itrs:600, metric:0.12525834115988094\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0810823890546031\n",
      "epoch:0, batch:649, loss:0.07966352254152298\n",
      "itrs:650, metric:0.11833384481466662\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08113565429424284\n",
      "epoch:0, batch:699, loss:0.0793333649635315\n",
      "itrs:700, metric:0.10798364568614842\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08087131983641403\n",
      "epoch:0, batch:749, loss:0.0787038803100586\n",
      "itrs:750, metric:0.11624067116183472\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08089923484380601\n",
      "epoch:0, batch:799, loss:0.07881377637386322\n",
      "itrs:800, metric:0.13069558204862106\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08116092179801\n",
      "epoch:0, batch:849, loss:0.0784970372915268\n",
      "itrs:850, metric:0.10991215180363523\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0811604724461789\n",
      "epoch:0, batch:899, loss:0.07828769087791443\n",
      "itrs:900, metric:0.1155303032833318\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08133912744952113\n",
      "epoch:0, batch:949, loss:0.07807815819978714\n",
      "itrs:950, metric:0.10884581682775106\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08135249699819258\n",
      "epoch:0, batch:999, loss:0.0780353844165802\n",
      "itrs:1000, metric:0.1153738474460321\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08162786995390298\n",
      "epoch:0, batch:1049, loss:0.07717075943946838\n",
      "itrs:1050, metric:0.10780975197211673\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.081659003805647\n",
      "epoch:0, batch:1099, loss:0.0776486024260521\n",
      "itrs:1100, metric:0.10942663202004042\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0815062440090118\n",
      "epoch:0, batch:1149, loss:0.07746089994907379\n",
      "itrs:1150, metric:0.1143829124193063\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.09434340101759107\n",
      "cost:1567.1079812049866s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08152693891623515\n",
      "epoch:1, batch:21, loss:0.07768817991018295\n",
      "itrs:1200, metric:0.11899846630565082\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08157900260057768\n",
      "epoch:1, batch:71, loss:0.07735671103000641\n",
      "itrs:1250, metric:0.11937200001529061\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08199866427154709\n",
      "epoch:1, batch:121, loss:0.07698263227939606\n",
      "itrs:1300, metric:0.10612998970147612\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0833700927593244\n",
      "epoch:1, batch:171, loss:0.07840383052825928\n",
      "itrs:1350, metric:0.08521040720976443\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.081961131461719\n",
      "epoch:1, batch:221, loss:0.07725992798805237\n",
      "itrs:1400, metric:0.10138523789199907\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08184476522877805\n",
      "epoch:1, batch:271, loss:0.0773242637515068\n",
      "itrs:1450, metric:0.12654214698861274\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08184819507245393\n",
      "epoch:1, batch:321, loss:0.07683244347572327\n",
      "itrs:1500, metric:0.14627518933515504\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08197702275336988\n",
      "epoch:1, batch:371, loss:0.07630634307861328\n",
      "itrs:1550, metric:0.13011493562011714\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0828589755594966\n",
      "epoch:1, batch:421, loss:0.07617305219173431\n",
      "itrs:1600, metric:0.118834874288354\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08270623669987034\n",
      "epoch:1, batch:471, loss:0.07640786468982697\n",
      "itrs:1650, metric:0.1233789306162584\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08241719998661774\n",
      "epoch:1, batch:521, loss:0.07618848979473114\n",
      "itrs:1700, metric:0.12825134094986826\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08280376844000917\n",
      "epoch:1, batch:571, loss:0.07661688327789307\n",
      "itrs:1750, metric:0.1333545546268788\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08365451262633548\n",
      "epoch:1, batch:621, loss:0.07583123445510864\n",
      "itrs:1800, metric:0.11773846648345877\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0835356428374899\n",
      "epoch:1, batch:671, loss:0.07597228139638901\n",
      "itrs:1850, metric:0.12716369023196167\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08452572675721373\n",
      "epoch:1, batch:721, loss:0.07614792883396149\n",
      "itrs:1900, metric:0.1316387683040239\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0836280713245164\n",
      "epoch:1, batch:771, loss:0.07623270153999329\n",
      "itrs:1950, metric:0.12676101354163113\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0843511749540555\n",
      "epoch:1, batch:821, loss:0.07496567070484161\n",
      "itrs:2000, metric:0.11857598547026606\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08135115209598204\n",
      "epoch:0, batch:599, loss:0.07842136919498444\n",
      "itrs:600, metric:0.13644418924716822\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08142102082781942\n",
      "epoch:0, batch:649, loss:0.07827422022819519\n",
      "itrs:650, metric:0.13212426386963266\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08231467754018856\n",
      "epoch:0, batch:699, loss:0.07893918454647064\n",
      "itrs:700, metric:0.10856653627479823\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08214939018250447\n",
      "epoch:0, batch:749, loss:0.07812894880771637\n",
      "itrs:750, metric:0.1164816674237904\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08268875733426227\n",
      "epoch:0, batch:799, loss:0.07868512719869614\n",
      "itrs:800, metric:0.0954815808567585\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08234729553752176\n",
      "epoch:0, batch:849, loss:0.0770348459482193\n",
      "itrs:850, metric:0.10356459499649494\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08253148804634274\n",
      "epoch:0, batch:899, loss:0.07702106237411499\n",
      "itrs:900, metric:0.10417715638393726\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08250498367002841\n",
      "epoch:0, batch:949, loss:0.07670260965824127\n",
      "itrs:950, metric:0.11159010708575402\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08237852170267973\n",
      "epoch:0, batch:999, loss:0.07637692242860794\n",
      "itrs:1000, metric:0.12133042654768865\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08357679669695402\n",
      "epoch:0, batch:1049, loss:0.07702486217021942\n",
      "itrs:1050, metric:0.12053649502523489\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08439722255524769\n",
      "epoch:0, batch:1099, loss:0.0770416259765625\n",
      "itrs:1100, metric:0.1258128652480284\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08313679806537244\n",
      "epoch:0, batch:1149, loss:0.07573115080595016\n",
      "itrs:1150, metric:0.11584815294112963\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08056044827411715\n",
      "cost:1545.503632068634s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08362560185185491\n",
      "epoch:1, batch:21, loss:0.07580580562353134\n",
      "itrs:1200, metric:0.13831510899239624\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08420365927549715\n",
      "epoch:1, batch:71, loss:0.07546395063400269\n",
      "itrs:1250, metric:0.11202460001707853\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08382469942006293\n",
      "epoch:1, batch:121, loss:0.07496240735054016\n",
      "itrs:1300, metric:0.1363096482482029\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08402023601956204\n",
      "epoch:1, batch:171, loss:0.07513676583766937\n",
      "itrs:1350, metric:0.1528854008566365\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08515684182778571\n",
      "epoch:1, batch:221, loss:0.07477247714996338\n",
      "itrs:1400, metric:0.13139093414751593\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08457400547182439\n",
      "epoch:1, batch:271, loss:0.07497183978557587\n",
      "itrs:1450, metric:0.11651235288458742\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0843622310590726\n",
      "epoch:1, batch:321, loss:0.07454417645931244\n",
      "itrs:1500, metric:0.11550970808550826\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08448109930785375\n",
      "epoch:1, batch:371, loss:0.07372550666332245\n",
      "itrs:1550, metric:0.13372537766658563\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08514441416457724\n",
      "epoch:1, batch:421, loss:0.07439552992582321\n",
      "itrs:1600, metric:0.11576484006720875\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08427390823850625\n",
      "epoch:1, batch:471, loss:0.0738266110420227\n",
      "itrs:1650, metric:0.13072529368010288\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0851145389918213\n",
      "epoch:1, batch:521, loss:0.07419880479574203\n",
      "itrs:1700, metric:0.13493327744714192\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08535183138493499\n",
      "epoch:1, batch:571, loss:0.0734991729259491\n",
      "itrs:1750, metric:0.12931533958150995\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08502450281896605\n",
      "epoch:1, batch:621, loss:0.0741415023803711\n",
      "itrs:1800, metric:0.14306027499500892\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08559585676574985\n",
      "epoch:1, batch:671, loss:0.07286065071821213\n",
      "itrs:1850, metric:0.11254024890521479\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0837137805351555\n",
      "epoch:1, batch:721, loss:0.0756920650601387\n",
      "itrs:1900, metric:0.11712800587326154\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08575935525927299\n",
      "epoch:1, batch:771, loss:0.07315438240766525\n",
      "itrs:1950, metric:0.12617708063298777\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08690885708319068\n",
      "epoch:1, batch:821, loss:0.07285988330841064\n",
      "itrs:2000, metric:0.13758497142583917\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08149653221887868\n",
      "epoch:0, batch:599, loss:0.0795171856880188\n",
      "itrs:600, metric:0.12027554440740763\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08142950596522043\n",
      "epoch:0, batch:649, loss:0.07797986268997192\n",
      "itrs:650, metric:0.11457561331598702\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08141143911126993\n",
      "epoch:0, batch:699, loss:0.07796142995357513\n",
      "itrs:700, metric:0.11572711085676228\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08179931559056243\n",
      "epoch:0, batch:749, loss:0.0782482847571373\n",
      "itrs:750, metric:0.11006814411683763\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08204480719637135\n",
      "epoch:0, batch:799, loss:0.07756276428699493\n",
      "itrs:800, metric:0.1127034184955619\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08241776515106235\n",
      "epoch:0, batch:849, loss:0.0782942920923233\n",
      "itrs:850, metric:0.10419924461941277\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08237485184618094\n",
      "epoch:0, batch:899, loss:0.0774611383676529\n",
      "itrs:900, metric:0.0809118029851763\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08247314867451516\n",
      "epoch:0, batch:949, loss:0.07769699394702911\n",
      "itrs:950, metric:0.11983512267331282\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0827666268581985\n",
      "epoch:0, batch:999, loss:0.07746550440788269\n",
      "itrs:1000, metric:0.08526031757008545\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0824077683607945\n",
      "epoch:0, batch:1049, loss:0.07675106823444366\n",
      "itrs:1050, metric:0.11058251393239718\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08265809618623154\n",
      "epoch:0, batch:1099, loss:0.07722190022468567\n",
      "itrs:1100, metric:0.1099851403580942\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08222023525191081\n",
      "epoch:0, batch:1149, loss:0.07611506432294846\n",
      "itrs:1150, metric:0.13448971169372173\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08318868074228743\n",
      "cost:1891.11039686203s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08285878495913901\n",
      "epoch:1, batch:21, loss:0.07651166617870331\n",
      "itrs:1200, metric:0.11649117771893441\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08333519724053783\n",
      "epoch:1, batch:71, loss:0.0759836807847023\n",
      "itrs:1250, metric:0.13783721616311798\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08287717388703301\n",
      "epoch:1, batch:121, loss:0.07587835937738419\n",
      "itrs:1300, metric:0.12685592818195096\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08288450371940298\n",
      "epoch:1, batch:171, loss:0.07621042430400848\n",
      "itrs:1350, metric:0.10666793526944378\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08345747456899298\n",
      "epoch:1, batch:221, loss:0.07598106563091278\n",
      "itrs:1400, metric:0.13573652307853384\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0831418820019523\n",
      "epoch:1, batch:271, loss:0.07611577212810516\n",
      "itrs:1450, metric:0.1260674628613439\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08321045914625853\n",
      "epoch:1, batch:321, loss:0.07545126974582672\n",
      "itrs:1500, metric:0.12357973904660312\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08353508859348517\n",
      "epoch:1, batch:371, loss:0.0752677470445633\n",
      "itrs:1550, metric:0.1223123744049038\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08387961842880133\n",
      "epoch:1, batch:421, loss:0.07513832300901413\n",
      "itrs:1600, metric:0.12013053620151293\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08331343331238576\n",
      "epoch:1, batch:471, loss:0.07472680509090424\n",
      "itrs:1650, metric:0.1403927561455854\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08390888192747532\n",
      "epoch:1, batch:521, loss:0.07501603662967682\n",
      "itrs:1700, metric:0.1351357096213835\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08405157260926369\n",
      "epoch:1, batch:571, loss:0.07504063844680786\n",
      "itrs:1750, metric:0.13432989391932196\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08438472415301657\n",
      "epoch:1, batch:621, loss:0.0745493471622467\n",
      "itrs:1800, metric:0.1399708245044495\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08421537782150737\n",
      "epoch:1, batch:671, loss:0.074261873960495\n",
      "itrs:1850, metric:0.13709745593622655\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08465166086009168\n",
      "epoch:1, batch:721, loss:0.07386477291584015\n",
      "itrs:1900, metric:0.13139672073158948\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08559002885990116\n",
      "epoch:1, batch:771, loss:0.0740530714392662\n",
      "itrs:1950, metric:0.12425385363234859\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08480724844923838\n",
      "epoch:1, batch:821, loss:0.07383416593074799\n",
      "itrs:2000, metric:0.12528497209071046\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08135388085609353\n",
      "epoch:0, batch:599, loss:0.07870373129844666\n",
      "itrs:600, metric:0.13072555601319338\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08190588703125747\n",
      "epoch:0, batch:649, loss:0.07853376865386963\n",
      "itrs:650, metric:0.12069785535168699\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08136482405737991\n",
      "epoch:0, batch:699, loss:0.0787668526172638\n",
      "itrs:700, metric:0.1303328239113398\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0819797750260761\n",
      "epoch:0, batch:749, loss:0.07800137996673584\n",
      "itrs:750, metric:0.11901913244335244\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08209311891002455\n",
      "epoch:0, batch:799, loss:0.07801341265439987\n",
      "itrs:800, metric:0.12038406760887788\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08187981632540972\n",
      "epoch:0, batch:849, loss:0.07800588756799698\n",
      "itrs:850, metric:0.1251920855000612\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08168409088121961\n",
      "epoch:0, batch:899, loss:0.07800211012363434\n",
      "itrs:900, metric:0.12698966164556522\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08248423749471723\n",
      "epoch:0, batch:949, loss:0.07775997370481491\n",
      "itrs:950, metric:0.10846506602982738\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08210162221414932\n",
      "epoch:0, batch:999, loss:0.07710739970207214\n",
      "itrs:1000, metric:0.12566061771950546\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08238428543536615\n",
      "epoch:0, batch:1049, loss:0.07692193239927292\n",
      "itrs:1050, metric:0.1286740420757421\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08290364082319979\n",
      "epoch:0, batch:1099, loss:0.07664313912391663\n",
      "itrs:1100, metric:0.1325421956904064\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08235940193776309\n",
      "epoch:0, batch:1149, loss:0.07751797884702682\n",
      "itrs:1150, metric:0.13295577304704947\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08523459962873549\n",
      "cost:1479.1557621955872s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08280103399009345\n",
      "epoch:1, batch:21, loss:0.07698477804660797\n",
      "itrs:1200, metric:0.10949903247951055\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0837564319388202\n",
      "epoch:1, batch:71, loss:0.07618848234415054\n",
      "itrs:1250, metric:0.1377961174489201\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08311108183238776\n",
      "epoch:1, batch:121, loss:0.0763179287314415\n",
      "itrs:1300, metric:0.13697048164933676\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08383027124943393\n",
      "epoch:1, batch:171, loss:0.07575711607933044\n",
      "itrs:1350, metric:0.1520124409130201\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08435248770511873\n",
      "epoch:1, batch:221, loss:0.07542138546705246\n",
      "itrs:1400, metric:0.12284946348496407\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0838455903999705\n",
      "epoch:1, batch:271, loss:0.07512271404266357\n",
      "itrs:1450, metric:0.1580335617930612\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08466654152447069\n",
      "epoch:1, batch:321, loss:0.07525455951690674\n",
      "itrs:1500, metric:0.12270914928906347\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08356968926268751\n",
      "epoch:1, batch:371, loss:0.07481342554092407\n",
      "itrs:1550, metric:0.15465931867034263\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08464223754175461\n",
      "epoch:1, batch:421, loss:0.07547873258590698\n",
      "itrs:1600, metric:0.10593330703904939\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08453193029177679\n",
      "epoch:1, batch:471, loss:0.07456277310848236\n",
      "itrs:1650, metric:0.14780388098269617\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08401244473197085\n",
      "epoch:1, batch:521, loss:0.07460547238588333\n",
      "itrs:1700, metric:0.14331676820668993\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08521098788675983\n",
      "epoch:1, batch:571, loss:0.07422669231891632\n",
      "itrs:1750, metric:0.13592628890845743\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08503902051195927\n",
      "epoch:1, batch:621, loss:0.0742093026638031\n",
      "itrs:1800, metric:0.13628633805423712\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08578775524943161\n",
      "epoch:1, batch:671, loss:0.07358650118112564\n",
      "itrs:1850, metric:0.12364373957675523\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08579871049553806\n",
      "epoch:1, batch:721, loss:0.07340718805789948\n",
      "itrs:1900, metric:0.16480348848260493\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08512164936197515\n",
      "epoch:1, batch:771, loss:0.07308107614517212\n",
      "itrs:1950, metric:0.1314250391920685\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08535144677752088\n",
      "epoch:1, batch:821, loss:0.07337816059589386\n",
      "itrs:2000, metric:0.15732775711115385\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0812588338382158\n",
      "epoch:0, batch:599, loss:0.07960371673107147\n",
      "itrs:600, metric:0.14505787128380357\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08096151032001295\n",
      "epoch:0, batch:649, loss:0.07890285551548004\n",
      "itrs:650, metric:0.14428648290000015\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08137533657331512\n",
      "epoch:0, batch:699, loss:0.07832522690296173\n",
      "itrs:700, metric:0.13376339152846412\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08142546608970766\n",
      "epoch:0, batch:749, loss:0.0782790333032608\n",
      "itrs:750, metric:0.14794997967383097\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08163916037170453\n",
      "epoch:0, batch:799, loss:0.07839256525039673\n",
      "itrs:800, metric:0.13083386346982784\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08192165711431237\n",
      "epoch:0, batch:849, loss:0.07880066335201263\n",
      "itrs:850, metric:0.13179597528956796\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08211583523180566\n",
      "epoch:0, batch:899, loss:0.07808990031480789\n",
      "itrs:900, metric:0.130754987141745\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08237620588431704\n",
      "epoch:0, batch:949, loss:0.07757988572120667\n",
      "itrs:950, metric:0.1287441261554199\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08269171842707286\n",
      "epoch:0, batch:999, loss:0.07715166360139847\n",
      "itrs:1000, metric:0.11745787095320945\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08175857030845023\n",
      "epoch:0, batch:1049, loss:0.07724994421005249\n",
      "itrs:1050, metric:0.12357583151343002\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08256516671749654\n",
      "epoch:0, batch:1099, loss:0.07775933295488358\n",
      "itrs:1100, metric:0.10909565934640948\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08280266697025483\n",
      "epoch:0, batch:1149, loss:0.07703731954097748\n",
      "itrs:1150, metric:0.13324228523256856\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.0885992841536114\n",
      "cost:1528.2930099964142s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0826994421158254\n",
      "epoch:1, batch:21, loss:0.07613926380872726\n",
      "itrs:1200, metric:0.14130063178271382\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08236718457158394\n",
      "epoch:1, batch:71, loss:0.07654031366109848\n",
      "itrs:1250, metric:0.14165150075408897\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08198066329446785\n",
      "epoch:1, batch:121, loss:0.07698395103216171\n",
      "itrs:1300, metric:0.14547043700885195\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08234329084992703\n",
      "epoch:1, batch:171, loss:0.07584542781114578\n",
      "itrs:1350, metric:0.14949106597525771\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0836389011086419\n",
      "epoch:1, batch:221, loss:0.07592836022377014\n",
      "itrs:1400, metric:0.1350533719006004\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08290229399323715\n",
      "epoch:1, batch:271, loss:0.07581023871898651\n",
      "itrs:1450, metric:0.14599824922037846\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08345531285352303\n",
      "epoch:1, batch:321, loss:0.07602737843990326\n",
      "itrs:1500, metric:0.134683965632153\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08308135210454254\n",
      "epoch:1, batch:371, loss:0.07623375207185745\n",
      "itrs:1550, metric:0.14414291241965882\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0835346041342684\n",
      "epoch:1, batch:421, loss:0.07577906548976898\n",
      "itrs:1600, metric:0.13807906440799797\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08312508312066073\n",
      "epoch:1, batch:471, loss:0.07605462521314621\n",
      "itrs:1650, metric:0.10052144746535355\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08319930934111766\n",
      "epoch:1, batch:521, loss:0.07673764228820801\n",
      "itrs:1700, metric:0.13065626630840496\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0844208306693682\n",
      "epoch:1, batch:571, loss:0.07546582818031311\n",
      "itrs:1750, metric:0.12426850526838583\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0836982983866904\n",
      "epoch:1, batch:621, loss:0.07552178204059601\n",
      "itrs:1800, metric:0.16160360462694776\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0840583218647637\n",
      "epoch:1, batch:671, loss:0.07439407706260681\n",
      "itrs:1850, metric:0.13052732953609494\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0853990099733367\n",
      "epoch:1, batch:721, loss:0.07528632879257202\n",
      "itrs:1900, metric:0.14920680282527127\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08482884177106266\n",
      "epoch:1, batch:771, loss:0.074849434196949\n",
      "itrs:1950, metric:0.151529622913285\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08377288383630369\n",
      "epoch:1, batch:821, loss:0.07450778782367706\n",
      "itrs:2000, metric:0.14445638134154956\n",
      "load done!\n",
      "Adjusting learning rate of group 0 to 3.0000e-02.\n",
      "Adjusting learning rate of group 1 to 3.0000e-02.\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08117436955459888\n",
      "epoch:0, batch:599, loss:0.07891343533992767\n",
      "itrs:600, metric:0.10488963201695595\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08112314461579306\n",
      "epoch:0, batch:649, loss:0.07871304452419281\n",
      "itrs:650, metric:0.114132872398952\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08185255377440936\n",
      "epoch:0, batch:699, loss:0.07849457114934921\n",
      "itrs:700, metric:0.10894079669716222\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08162388144205207\n",
      "epoch:0, batch:749, loss:0.07809469103813171\n",
      "itrs:750, metric:0.10755343324523588\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08227717213547218\n",
      "epoch:0, batch:799, loss:0.07799404859542847\n",
      "itrs:800, metric:0.11756517832771654\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08152484540417917\n",
      "epoch:0, batch:849, loss:0.07770469039678574\n",
      "itrs:850, metric:0.11772942821056379\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08370233540293295\n",
      "epoch:0, batch:899, loss:0.07698210328817368\n",
      "itrs:900, metric:0.11079087624532348\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08165994887400063\n",
      "epoch:0, batch:949, loss:0.0775538980960846\n",
      "itrs:950, metric:0.11384978944725944\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08282484589851058\n",
      "epoch:0, batch:999, loss:0.07762183994054794\n",
      "itrs:1000, metric:0.10617761905684737\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08227836412885202\n",
      "epoch:0, batch:1049, loss:0.07754512131214142\n",
      "itrs:1050, metric:0.11116277792182877\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08228161841210227\n",
      "epoch:0, batch:1099, loss:0.07657875120639801\n",
      "itrs:1100, metric:0.10987172826748974\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08363344535864585\n",
      "epoch:0, batch:1149, loss:0.07681484520435333\n",
      "itrs:1150, metric:0.11791703909831186\n",
      "Adjusting learning rate of group 0 to 2.7000e-02.\n",
      "Adjusting learning rate of group 1 to 2.7000e-02.\n",
      "loss:0.08363584423100583\n",
      "cost:1504.1125078201294s\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08280682423439\n",
      "epoch:1, batch:21, loss:0.07667140662670135\n",
      "itrs:1200, metric:0.1154179655482671\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08290776794505911\n",
      "epoch:1, batch:71, loss:0.07666745781898499\n",
      "itrs:1250, metric:0.13165295880882394\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08215334837494294\n",
      "epoch:1, batch:121, loss:0.07610884308815002\n",
      "itrs:1300, metric:0.13424664822686685\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0828075237073658\n",
      "epoch:1, batch:171, loss:0.07584914565086365\n",
      "itrs:1350, metric:0.13146317735694812\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08303163343534416\n",
      "epoch:1, batch:221, loss:0.07599080353975296\n",
      "itrs:1400, metric:0.12791667900902312\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0832208926988294\n",
      "epoch:1, batch:271, loss:0.07563796639442444\n",
      "itrs:1450, metric:0.1278292039341952\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08370455731015118\n",
      "epoch:1, batch:321, loss:0.0759023129940033\n",
      "itrs:1500, metric:0.12122759908682686\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08372008257693683\n",
      "epoch:1, batch:371, loss:0.07508658617734909\n",
      "itrs:1550, metric:0.11803517493839434\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08382045479443394\n",
      "epoch:1, batch:421, loss:0.07495136559009552\n",
      "itrs:1600, metric:0.12144437858988164\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08387322826937779\n",
      "epoch:1, batch:471, loss:0.07576093822717667\n",
      "itrs:1650, metric:0.11986368718459693\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08192304742679474\n",
      "epoch:1, batch:521, loss:0.07681669294834137\n",
      "itrs:1700, metric:0.12941849798151753\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08371884234723873\n",
      "epoch:1, batch:571, loss:0.07530106604099274\n",
      "itrs:1750, metric:0.12320161347613394\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08571161709673576\n",
      "epoch:1, batch:621, loss:0.07539993524551392\n",
      "itrs:1800, metric:0.11075898662795426\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.0843082139938546\n",
      "epoch:1, batch:671, loss:0.0747118815779686\n",
      "itrs:1850, metric:0.1365927857014943\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08308189252947058\n",
      "epoch:1, batch:721, loss:0.07668185234069824\n",
      "itrs:1900, metric:0.11809257916978091\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08491788929262784\n",
      "epoch:1, batch:771, loss:0.07459329068660736\n",
      "itrs:1950, metric:0.13945681235888463\n",
      "valid dataloader: \n",
      "0/310\n",
      "100/310\n",
      "200/310\n",
      "300/310\n",
      "309/310\n",
      "valid loss:0.08395963456266743\n",
      "epoch:1, batch:821, loss:0.07454407215118408\n",
      "itrs:2000, metric:0.12600467377861704\n"
     ]
    }
   ],
   "source": [
    "save_path = '/mnt/beegfs/strategy_intern/zzdai_intern/for_wpxu/nn_results/mlp_baseline'\n",
    "for i in range(5):\n",
    "    model_type = 'mlp' \n",
    "    wd = 1e-8\n",
    "    itrs = 2000\n",
    "    num_workers = 4\n",
    "    batch_size = 2**16\n",
    "    ## 创建存放模型的experiment_path\n",
    "    experiment_path = os.path.join(save_path, model_type+'_'+str(i))\n",
    "    if not os.path.exists(experiment_path):\n",
    "        os.makedirs(experiment_path)\n",
    "    ## 根据model_type进行模型的初始化\n",
    "    net = eval(model_type)(input_size=len(feas))\n",
    "    net = net.to(device)\n",
    "    data_tmp = data_train.sample(frac=0.80, axis=0).sort_index().reset_index(drop=True)\n",
    "    x_train_tensor, y_train_tensor = torch.tensor(np.array(data_tmp[feas]), dtype=torch.float32), torch.tensor(np.array(data_tmp['y1']), dtype=torch.float32)\n",
    "    dataset_train = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    print('load done!')\n",
    "    \n",
    "    parameters = get_parameter_groups(net, weight_decay=wd) # 对模型进行分组正则\n",
    "    opt_args = dict(lr=3e-2, weight_decay=wd) # 优化器参数\n",
    "    optim = torch.optim.Adam(parameters, **opt_args) # 优化器初始化\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9, verbose=True) # 优化器的schedule\n",
    "    train(itrs, net, dataloader_train, optim, device, dataloader_valid, y_valid, experiment_path, scheduler=scheduler, interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "568415f8034d4245baef1815eee350d86d872e0622f7175a8825f29df393c837"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
